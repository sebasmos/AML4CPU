{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d549302b-d363-432e-ab9d-56431c33eecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_dir = \"../outputs/exp2_init_models/exp2_init_models\"\n",
    "# root_dir = \"../../outputs/exp1_all\"\n",
    "root_dir = \"../../outputs/Exp3/exp3_merged\"\n",
    "# root_dir = \"../../outputs/Exp3\"\n",
    "exp_name = \"exp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "07f07ce5-4ced-499a-ad9b-2b39d66f50cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def process_model_directories(root_dir, lista_de_modelos,exp_name):\n",
    "    MAE_values = []\n",
    "    RMSE_values = []\n",
    "    SMAPE_values = []\n",
    "    r2_values = []\n",
    "    MASE_values = []\n",
    "    training_time_values = []\n",
    "    inference_time_values = []\n",
    "    model_memory_values = []\n",
    "\n",
    "    for subdir in os.listdir(root_dir):\n",
    "        subdir_path = os.path.join(root_dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for model_dir in lista_de_modelos:\n",
    "                model_dir_path = os.path.join(subdir_path, model_dir)\n",
    "                if os.path.isdir(model_dir_path):\n",
    "                    if exp_name == \"exp3\":\n",
    "                        try:\n",
    "                            for file_name in os.listdir(model_dir_path):\n",
    "                                if \"model_metrics\" in file_name and file_name.endswith('.csv'):\n",
    "                                    model_metrics_path = os.path.join(model_dir_path, file_name)\n",
    "                                    if os.path.isfile(model_metrics_path):\n",
    "                                                model_metrics = pd.read_csv(model_metrics_path)\n",
    "                                                rename_dict = {\n",
    "                                                        'mae': 'MAE',\n",
    "                                                        'rmse': 'RMSE',\n",
    "                                                        'r2': 'r2',\n",
    "                                                        'smape': 'SMAPE',\n",
    "                                                        'mase': 'MASE',\n",
    "                                                        'Training Time': 'Training_time',\n",
    "                                                        'Inference Time': 'Inference_time',\n",
    "                                                        'Model memory (MB)': 'Model memory (MB)'\n",
    "                                                    }\n",
    "                                                model_metrics.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {e} and {model_metrics_path} does not exist\")\n",
    "                    else:\n",
    "                        model_metrics_path = os.path.join(model_dir_path, \"model_data.csv\")\n",
    "                        model_metrics = pd.read_csv(model_metrics_path)\n",
    "                    MAE_values.append(float(model_metrics.MAE))\n",
    "                    RMSE_values.append(float(model_metrics['RMSE']))\n",
    "                    SMAPE_values.append(float(model_metrics['SMAPE']))\n",
    "                    r2_values.append(float(model_metrics['r2']))\n",
    "                    MASE_values.append(float(model_metrics['MASE']))\n",
    "                    training_time_values.append(float(model_metrics['Training_time']))\n",
    "                    inference_time_values.append(float(model_metrics['Inference_time']))\n",
    "                    model_memory_values.append(float(model_metrics['Model memory (MB)']))\n",
    "        \n",
    "    MAE_array = np.array(MAE_values)\n",
    "    RMSE_array = np.array(RMSE_values)\n",
    "    SMAPE_array = np.array(SMAPE_values)\n",
    "    r2_array = np.array(r2_values)\n",
    "    MASE_array = np.array(MASE_values)\n",
    "    training_time_array = np.array(training_time_values)\n",
    "    inference_time_array = np.array(inference_time_values)\n",
    "    model_memory_array = np.array(model_memory_values)\n",
    "    print(f\"processed {len(model_memory_array)} seeds\")\n",
    "    return (MAE_array, RMSE_array, SMAPE_array, r2_array, MASE_array,\n",
    "            training_time_array, inference_time_array, model_memory_array)\n",
    "def arrays_to_dataframe(MAE_array, RMSE_array, SMAPE_array, r2_array, MASE_array,\n",
    "                        training_time_array, inference_time_array, model_memory_array):\n",
    "    SMALL_CONSTANT = 1\n",
    "    log_training_time = np.log(training_time_array + SMALL_CONSTANT)\n",
    "    log_inference_time = np.log(inference_time_array + SMALL_CONSTANT)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'MAE': MAE_array,\n",
    "        'RMSE': RMSE_array,\n",
    "        'SMAPE': SMAPE_array,\n",
    "        'r2': r2_array,\n",
    "        'MASE': MASE_array,\n",
    "        'Training_time': training_time_array,\n",
    "        'Training_time_log': log_training_time,\n",
    "        'Inference_time': inference_time_array,\n",
    "        'Inference_time_log': log_inference_time,\n",
    "        'Model_memory': model_memory_array\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "import pandas as pd\n",
    "\n",
    "def extract_window_size(model_name):\n",
    "    parts = model_name.split('_ws_')\n",
    "    if len(parts) > 1:\n",
    "        try:\n",
    "            window_size = int(parts[1])\n",
    "            return parts[0], window_size\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return model_name, None\n",
    "\n",
    "def add_window_size_column(df):\n",
    "    if 'Window Size' not in df.columns:\n",
    "        df[['Model', 'Window Size']] = df['Model'].apply(extract_window_size).apply(pd.Series)\n",
    "    return df\n",
    "\n",
    "def update_model_names(df, name_mapping):\n",
    "    df['Model'] = df['Model'].replace(name_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118cb6e-be72-49d5-b76d-e539124cff11",
   "metadata": {},
   "source": [
    "## Generate csv for every seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6dd0d544-f075-44b6-940b-00a9bc0e3ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Finetuned_128CL_True_RoPE_tested_on_64CL',\n",
       " 'Finetuned_256CL_False_RoPE_tested_on_64CL',\n",
       " 'Finetuned_32CL_False_RoPE_tested_on_32CL',\n",
       " 'Finetuned_128CL_False_RoPE_tested_on_128CL',\n",
       " 'Finetuned_128CL_False_RoPE_tested_on_256CL',\n",
       " 'Finetuned_256CL_True_RoPE_tested_on_256CL',\n",
       " 'Finetuned_32CL_True_RoPE_tested_on_32CL',\n",
       " 'Finetuned_128CL_True_RoPE_tested_on_32CL',\n",
       " 'Finetuned_32CL_False_RoPE_tested_on_256CL',\n",
       " 'Finetuned_256CL_True_RoPE_tested_on_32CL',\n",
       " 'Finetuned_64CL_True_RoPE_tested_on_64CL',\n",
       " 'Finetuned_128CL_True_RoPE_tested_on_256CL',\n",
       " 'Finetuned_32CL_False_RoPE_tested_on_128CL',\n",
       " 'Finetuned_32CL_True_RoPE_tested_on_256CL',\n",
       " 'Finetuned_128CL_False_RoPE_tested_on_64CL',\n",
       " 'Finetuned_256CL_False_RoPE_tested_on_256CL',\n",
       " 'Finetuned_64CL_True_RoPE_tested_on_32CL',\n",
       " 'Finetuned_32CL_False_RoPE_tested_on_64CL',\n",
       " 'Finetuned_64CL_True_RoPE_tested_on_256CL',\n",
       " 'Finetuned_256CL_False_RoPE_tested_on_32CL',\n",
       " 'Finetuned_128CL_True_RoPE_tested_on_128CL',\n",
       " 'Finetuned_256CL_True_RoPE_tested_on_128CL',\n",
       " 'Finetuned_256CL_False_RoPE_tested_on_128CL',\n",
       " 'Finetuned_32CL_True_RoPE_tested_on_128CL',\n",
       " 'Finetuned_32CL_True_RoPE_tested_on_64CL',\n",
       " 'Finetuned_128CL_False_RoPE_tested_on_32CL',\n",
       " 'Finetuned_64CL_True_RoPE_tested_on_128CL',\n",
       " 'Finetuned_256CL_True_RoPE_tested_on_64CL']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir_exp1 = os.path.join(root_dir,\"testbed_0\")# because model names are inside seed folders\n",
    "folder_names = []\n",
    "\n",
    "# Iterate through each item in the directory\n",
    "for item in os.listdir(root_dir_exp1):\n",
    "    if os.path.isdir(os.path.join(root_dir_exp1, item)):\n",
    "\n",
    "        folder_names.append(item)\n",
    "folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b5cd8336-9d0b-4313-9dbb-c7d4dd06ff5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 16 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 16 seeds\n",
      "processed 20 seeds\n",
      "processed 15 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 20 seeds\n",
      "processed 15 seeds\n",
      "processed 20 seeds\n",
      "['Finetuned_128CL_True_RoPE_tested_on_64CL'\n",
      " 'Finetuned_256CL_False_RoPE_tested_on_64CL'\n",
      " 'Finetuned_32CL_False_RoPE_tested_on_32CL'\n",
      " 'Finetuned_128CL_False_RoPE_tested_on_128CL'\n",
      " 'Finetuned_128CL_False_RoPE_tested_on_256CL'\n",
      " 'Finetuned_256CL_True_RoPE_tested_on_256CL'\n",
      " 'Finetuned_32CL_True_RoPE_tested_on_32CL'\n",
      " 'Finetuned_128CL_True_RoPE_tested_on_32CL'\n",
      " 'Finetuned_32CL_False_RoPE_tested_on_256CL'\n",
      " 'Finetuned_256CL_True_RoPE_tested_on_32CL'\n",
      " 'Finetuned_64CL_True_RoPE_tested_on_64CL'\n",
      " 'Finetuned_128CL_True_RoPE_tested_on_256CL'\n",
      " 'Finetuned_32CL_False_RoPE_tested_on_128CL'\n",
      " 'Finetuned_32CL_True_RoPE_tested_on_256CL'\n",
      " 'Finetuned_128CL_False_RoPE_tested_on_64CL'\n",
      " 'Finetuned_256CL_False_RoPE_tested_on_256CL'\n",
      " 'Finetuned_64CL_True_RoPE_tested_on_32CL'\n",
      " 'Finetuned_32CL_False_RoPE_tested_on_64CL'\n",
      " 'Finetuned_64CL_True_RoPE_tested_on_256CL'\n",
      " 'Finetuned_256CL_False_RoPE_tested_on_32CL'\n",
      " 'Finetuned_128CL_True_RoPE_tested_on_128CL'\n",
      " 'Finetuned_256CL_True_RoPE_tested_on_128CL'\n",
      " 'Finetuned_256CL_False_RoPE_tested_on_128CL'\n",
      " 'Finetuned_32CL_True_RoPE_tested_on_128CL'\n",
      " 'Finetuned_32CL_True_RoPE_tested_on_64CL'\n",
      " 'Finetuned_128CL_False_RoPE_tested_on_32CL'\n",
      " 'Finetuned_64CL_True_RoPE_tested_on_128CL'\n",
      " 'Finetuned_256CL_True_RoPE_tested_on_64CL']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_window_size(model_name):\n",
    "    parts = model_name.split('_ws_')\n",
    "    if len(parts) > 1:\n",
    "        try:\n",
    "            return parts[0], int(parts[1])\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return model_name, None\n",
    "\n",
    "def add_window_size_column(df):\n",
    "    df[['Model', 'Window Size']] = df['Model'].apply(extract_window_size).apply(pd.Series)\n",
    "    return df\n",
    "\n",
    "def update_model_names(df, name_mapping):\n",
    "    df['Model'] = df['Model'].replace(name_mapping)\n",
    "    return df\n",
    "\n",
    "# Initialize a list to collect dataframes\n",
    "all_metrics = []\n",
    "\n",
    "# Iterate through each model to process directories and collect metrics\n",
    "for model_name in folder_names:\n",
    "    lista_de_modelos = [f\"{model_name}\"]\n",
    "    metrics_arrays = process_model_directories(root_dir, lista_de_modelos, exp_name)\n",
    "    df = arrays_to_dataframe(*metrics_arrays)\n",
    "    df[\"Model\"] = model_name\n",
    "    all_metrics.append(df)\n",
    "\n",
    "\n",
    "all_models_df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "all_models_df.rename(columns={\n",
    "    'Inference_time': 'Inference Time',\n",
    "    'Training_time': 'Training Time',\n",
    "    'Model_memory':'Model memory (MB)'\n",
    "}, inplace=True)\n",
    "\n",
    "all_models_df = add_window_size_column(all_models_df)\n",
    "print(all_models_df.Model.unique())\n",
    "if exp_name == \"exp2\":\n",
    "    name_mapping = {\n",
    "        'AdaptiveRandomForest': 'Adaptive Random Forest (ARF)',\n",
    "        'HoeffdingAdaptiveTreeRegressor': 'Hoeffding Adaptive Tree Regressor',\n",
    "        'HoeffdingTreeRegressor': 'Hoeffding Tree Regressor',\n",
    "        'MLP_partialfit': 'MLP partialfit',\n",
    "        'PassiveAggressive': 'Passive Aggressive (PA)',\n",
    "        'SGDRegressor': 'SGD Regressor',\n",
    "        'SRPRegressor': 'SRP Regressor',\n",
    "        'XGBRegressor': 'XGBoost Regressor',\n",
    "    }\n",
    "    all_models_df = update_model_names(all_models_df, name_mapping)\n",
    "elif exp_name==\"exp1\":\n",
    "    name_mapping = {\n",
    "        'SVR': 'Support Vector Regressor (SVR)',\n",
    "        'XGBRegressor': 'XGBoost Regressor',\n",
    "        'RandomForestRegressor': 'Random Forest',\n",
    "        'LinearRegression': 'Linear Regression (LR)',\n",
    "        'AdaBoostRegressor': 'Ada Boost Regressor',\n",
    "        'DecisionTreeRegressor': 'Decision Tree Regressor',\n",
    "        'MLPRegressor': 'MLP partialfit',\n",
    "        'PassiveAggressiveRegressor': 'Passive Aggressive (PA)',\n",
    "        'KNeighborsRegressor': 'K-Neighbors Regressor',\n",
    "        'LSTM': 'LSTM',\n",
    "        'GRU': 'GRU',\n",
    "        'SGDRegressor': 'SGD Regressor',\n",
    "        'BI-LSTM': 'BI-LSTM',\n",
    "        'LSTM_ATTN': 'LSTM with Attention'\n",
    "    }\n",
    "    all_models_df = update_model_names(all_models_df, name_mapping)\n",
    "\n",
    "    all_models_df = all_models_df[all_models_df['Model'] != 'MLP partialfit']\n",
    "    all_models_df = all_models_df[all_models_df['Model'] != 'K-Neighbors Regressor']\n",
    "\n",
    "all_models_df.to_csv(f\"../../outputs/{exp_name}_full_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "99cc3a1a-d9f3-453a-a56d-ea066b1fc5de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>SMAPE</th>\n",
       "      <th>r2</th>\n",
       "      <th>MASE</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Training_time_log</th>\n",
       "      <th>Inference Time</th>\n",
       "      <th>Inference_time_log</th>\n",
       "      <th>Model memory (MB)</th>\n",
       "      <th>Model</th>\n",
       "      <th>Window Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.979095</td>\n",
       "      <td>8.427499</td>\n",
       "      <td>23.472632</td>\n",
       "      <td>0.913205</td>\n",
       "      <td>1.125024</td>\n",
       "      <td>1470.660387</td>\n",
       "      <td>7.294147</td>\n",
       "      <td>76.969064</td>\n",
       "      <td>4.356312</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.061859</td>\n",
       "      <td>8.177308</td>\n",
       "      <td>24.805921</td>\n",
       "      <td>0.910219</td>\n",
       "      <td>1.605502</td>\n",
       "      <td>1467.349281</td>\n",
       "      <td>7.291894</td>\n",
       "      <td>76.777868</td>\n",
       "      <td>4.353857</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.735010</td>\n",
       "      <td>8.081811</td>\n",
       "      <td>24.093711</td>\n",
       "      <td>0.911998</td>\n",
       "      <td>1.305857</td>\n",
       "      <td>1478.667276</td>\n",
       "      <td>7.299573</td>\n",
       "      <td>76.599787</td>\n",
       "      <td>4.351565</td>\n",
       "      <td>0.316505</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.736139</td>\n",
       "      <td>10.110521</td>\n",
       "      <td>26.056445</td>\n",
       "      <td>0.867899</td>\n",
       "      <td>2.175479</td>\n",
       "      <td>1469.414234</td>\n",
       "      <td>7.293299</td>\n",
       "      <td>76.758397</td>\n",
       "      <td>4.353607</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.198985</td>\n",
       "      <td>8.101142</td>\n",
       "      <td>22.758916</td>\n",
       "      <td>0.917276</td>\n",
       "      <td>1.163660</td>\n",
       "      <td>1466.501148</td>\n",
       "      <td>7.291316</td>\n",
       "      <td>76.607342</td>\n",
       "      <td>4.351662</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>4.684654</td>\n",
       "      <td>9.123501</td>\n",
       "      <td>23.969622</td>\n",
       "      <td>0.895643</td>\n",
       "      <td>1.212260</td>\n",
       "      <td>1477.973678</td>\n",
       "      <td>7.299104</td>\n",
       "      <td>136.439170</td>\n",
       "      <td>4.923181</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>3.833669</td>\n",
       "      <td>7.984131</td>\n",
       "      <td>22.693442</td>\n",
       "      <td>0.923137</td>\n",
       "      <td>1.175680</td>\n",
       "      <td>1478.619176</td>\n",
       "      <td>7.299540</td>\n",
       "      <td>136.345406</td>\n",
       "      <td>4.922499</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>4.502005</td>\n",
       "      <td>8.037910</td>\n",
       "      <td>23.373216</td>\n",
       "      <td>0.917776</td>\n",
       "      <td>1.260558</td>\n",
       "      <td>1481.795525</td>\n",
       "      <td>7.301684</td>\n",
       "      <td>136.539286</td>\n",
       "      <td>4.923910</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>6.104835</td>\n",
       "      <td>11.565793</td>\n",
       "      <td>27.485174</td>\n",
       "      <td>0.829384</td>\n",
       "      <td>1.386435</td>\n",
       "      <td>1481.444956</td>\n",
       "      <td>7.301448</td>\n",
       "      <td>136.494026</td>\n",
       "      <td>4.923580</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>4.053947</td>\n",
       "      <td>7.946122</td>\n",
       "      <td>23.910736</td>\n",
       "      <td>0.919341</td>\n",
       "      <td>1.294765</td>\n",
       "      <td>1482.200169</td>\n",
       "      <td>7.301957</td>\n",
       "      <td>136.517484</td>\n",
       "      <td>4.923751</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>542 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MAE       RMSE      SMAPE        r2      MASE  Training Time  \\\n",
       "0    3.979095   8.427499  23.472632  0.913205  1.125024    1470.660387   \n",
       "1    5.061859   8.177308  24.805921  0.910219  1.605502    1467.349281   \n",
       "2    4.735010   8.081811  24.093711  0.911998  1.305857    1478.667276   \n",
       "3    5.736139  10.110521  26.056445  0.867899  2.175479    1469.414234   \n",
       "4    4.198985   8.101142  22.758916  0.917276  1.163660    1466.501148   \n",
       "..        ...        ...        ...       ...       ...            ...   \n",
       "537  4.684654   9.123501  23.969622  0.895643  1.212260    1477.973678   \n",
       "538  3.833669   7.984131  22.693442  0.923137  1.175680    1478.619176   \n",
       "539  4.502005   8.037910  23.373216  0.917776  1.260558    1481.795525   \n",
       "540  6.104835  11.565793  27.485174  0.829384  1.386435    1481.444956   \n",
       "541  4.053947   7.946122  23.910736  0.919341  1.294765    1482.200169   \n",
       "\n",
       "     Training_time_log  Inference Time  Inference_time_log  Model memory (MB)  \\\n",
       "0             7.294147       76.969064            4.356312           0.316566   \n",
       "1             7.291894       76.777868            4.353857           0.316566   \n",
       "2             7.299573       76.599787            4.351565           0.316505   \n",
       "3             7.293299       76.758397            4.353607           0.316566   \n",
       "4             7.291316       76.607342            4.351662           0.316566   \n",
       "..                 ...             ...                 ...                ...   \n",
       "537           7.299104      136.439170            4.923181           0.316566   \n",
       "538           7.299540      136.345406            4.922499           0.316566   \n",
       "539           7.301684      136.539286            4.923910           0.316566   \n",
       "540           7.301448      136.494026            4.923580           0.316566   \n",
       "541           7.301957      136.517484            4.923751           0.316566   \n",
       "\n",
       "                                        Model Window Size  \n",
       "0    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "1    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "2    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "3    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "4    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "..                                        ...         ...  \n",
       "537  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "538  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "539  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "540  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "541  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "\n",
       "[542 rows x 12 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1c17fd5b-1e39-484d-b4ee-f26c2f996ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Finetuned_128CL_True_RoPE_tested_on_64CL',\n",
       "       'Finetuned_256CL_False_RoPE_tested_on_64CL',\n",
       "       'Finetuned_32CL_False_RoPE_tested_on_32CL',\n",
       "       'Finetuned_128CL_False_RoPE_tested_on_128CL',\n",
       "       'Finetuned_128CL_False_RoPE_tested_on_256CL',\n",
       "       'Finetuned_256CL_True_RoPE_tested_on_256CL',\n",
       "       'Finetuned_32CL_True_RoPE_tested_on_32CL',\n",
       "       'Finetuned_128CL_True_RoPE_tested_on_32CL',\n",
       "       'Finetuned_32CL_False_RoPE_tested_on_256CL',\n",
       "       'Finetuned_256CL_True_RoPE_tested_on_32CL',\n",
       "       'Finetuned_64CL_True_RoPE_tested_on_64CL',\n",
       "       'Finetuned_128CL_True_RoPE_tested_on_256CL',\n",
       "       'Finetuned_32CL_False_RoPE_tested_on_128CL',\n",
       "       'Finetuned_32CL_True_RoPE_tested_on_256CL',\n",
       "       'Finetuned_128CL_False_RoPE_tested_on_64CL',\n",
       "       'Finetuned_256CL_False_RoPE_tested_on_256CL',\n",
       "       'Finetuned_64CL_True_RoPE_tested_on_32CL',\n",
       "       'Finetuned_32CL_False_RoPE_tested_on_64CL',\n",
       "       'Finetuned_64CL_True_RoPE_tested_on_256CL',\n",
       "       'Finetuned_256CL_False_RoPE_tested_on_32CL',\n",
       "       'Finetuned_128CL_True_RoPE_tested_on_128CL',\n",
       "       'Finetuned_256CL_True_RoPE_tested_on_128CL',\n",
       "       'Finetuned_256CL_False_RoPE_tested_on_128CL',\n",
       "       'Finetuned_32CL_True_RoPE_tested_on_128CL',\n",
       "       'Finetuned_32CL_True_RoPE_tested_on_64CL',\n",
       "       'Finetuned_128CL_False_RoPE_tested_on_32CL',\n",
       "       'Finetuned_64CL_True_RoPE_tested_on_128CL',\n",
       "       'Finetuned_256CL_True_RoPE_tested_on_64CL'], dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_df.Model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "74fcd21b-091a-4322-95c9-f72914d4aa11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_models_df[(all_models_df['Model'] == \"Adaptive Random Forest (ARF)\") & (all_models_df['Window Size'] == 64)][[\"MAE\",\"Training Time\",\"Inference Time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4e6beb3a-e040-49e2-b462-50d361eb9dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>SMAPE</th>\n",
       "      <th>r2</th>\n",
       "      <th>MASE</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Training_time_log</th>\n",
       "      <th>Inference Time</th>\n",
       "      <th>Inference_time_log</th>\n",
       "      <th>Model memory (MB)</th>\n",
       "      <th>Model</th>\n",
       "      <th>Window Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.979095</td>\n",
       "      <td>8.427499</td>\n",
       "      <td>23.472632</td>\n",
       "      <td>0.913205</td>\n",
       "      <td>1.125024</td>\n",
       "      <td>1470.660387</td>\n",
       "      <td>7.294147</td>\n",
       "      <td>76.969064</td>\n",
       "      <td>4.356312</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.061859</td>\n",
       "      <td>8.177308</td>\n",
       "      <td>24.805921</td>\n",
       "      <td>0.910219</td>\n",
       "      <td>1.605502</td>\n",
       "      <td>1467.349281</td>\n",
       "      <td>7.291894</td>\n",
       "      <td>76.777868</td>\n",
       "      <td>4.353857</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.735010</td>\n",
       "      <td>8.081811</td>\n",
       "      <td>24.093711</td>\n",
       "      <td>0.911998</td>\n",
       "      <td>1.305857</td>\n",
       "      <td>1478.667276</td>\n",
       "      <td>7.299573</td>\n",
       "      <td>76.599787</td>\n",
       "      <td>4.351565</td>\n",
       "      <td>0.316505</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.736139</td>\n",
       "      <td>10.110521</td>\n",
       "      <td>26.056445</td>\n",
       "      <td>0.867899</td>\n",
       "      <td>2.175479</td>\n",
       "      <td>1469.414234</td>\n",
       "      <td>7.293299</td>\n",
       "      <td>76.758397</td>\n",
       "      <td>4.353607</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.198985</td>\n",
       "      <td>8.101142</td>\n",
       "      <td>22.758916</td>\n",
       "      <td>0.917276</td>\n",
       "      <td>1.163660</td>\n",
       "      <td>1466.501148</td>\n",
       "      <td>7.291316</td>\n",
       "      <td>76.607342</td>\n",
       "      <td>4.351662</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>4.684654</td>\n",
       "      <td>9.123501</td>\n",
       "      <td>23.969622</td>\n",
       "      <td>0.895643</td>\n",
       "      <td>1.212260</td>\n",
       "      <td>1477.973678</td>\n",
       "      <td>7.299104</td>\n",
       "      <td>136.439170</td>\n",
       "      <td>4.923181</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>3.833669</td>\n",
       "      <td>7.984131</td>\n",
       "      <td>22.693442</td>\n",
       "      <td>0.923137</td>\n",
       "      <td>1.175680</td>\n",
       "      <td>1478.619176</td>\n",
       "      <td>7.299540</td>\n",
       "      <td>136.345406</td>\n",
       "      <td>4.922499</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>4.502005</td>\n",
       "      <td>8.037910</td>\n",
       "      <td>23.373216</td>\n",
       "      <td>0.917776</td>\n",
       "      <td>1.260558</td>\n",
       "      <td>1481.795525</td>\n",
       "      <td>7.301684</td>\n",
       "      <td>136.539286</td>\n",
       "      <td>4.923910</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>6.104835</td>\n",
       "      <td>11.565793</td>\n",
       "      <td>27.485174</td>\n",
       "      <td>0.829384</td>\n",
       "      <td>1.386435</td>\n",
       "      <td>1481.444956</td>\n",
       "      <td>7.301448</td>\n",
       "      <td>136.494026</td>\n",
       "      <td>4.923580</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>4.053947</td>\n",
       "      <td>7.946122</td>\n",
       "      <td>23.910736</td>\n",
       "      <td>0.919341</td>\n",
       "      <td>1.294765</td>\n",
       "      <td>1482.200169</td>\n",
       "      <td>7.301957</td>\n",
       "      <td>136.517484</td>\n",
       "      <td>4.923751</td>\n",
       "      <td>0.316566</td>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>542 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MAE       RMSE      SMAPE        r2      MASE  Training Time  \\\n",
       "0    3.979095   8.427499  23.472632  0.913205  1.125024    1470.660387   \n",
       "1    5.061859   8.177308  24.805921  0.910219  1.605502    1467.349281   \n",
       "2    4.735010   8.081811  24.093711  0.911998  1.305857    1478.667276   \n",
       "3    5.736139  10.110521  26.056445  0.867899  2.175479    1469.414234   \n",
       "4    4.198985   8.101142  22.758916  0.917276  1.163660    1466.501148   \n",
       "..        ...        ...        ...       ...       ...            ...   \n",
       "537  4.684654   9.123501  23.969622  0.895643  1.212260    1477.973678   \n",
       "538  3.833669   7.984131  22.693442  0.923137  1.175680    1478.619176   \n",
       "539  4.502005   8.037910  23.373216  0.917776  1.260558    1481.795525   \n",
       "540  6.104835  11.565793  27.485174  0.829384  1.386435    1481.444956   \n",
       "541  4.053947   7.946122  23.910736  0.919341  1.294765    1482.200169   \n",
       "\n",
       "     Training_time_log  Inference Time  Inference_time_log  Model memory (MB)  \\\n",
       "0             7.294147       76.969064            4.356312           0.316566   \n",
       "1             7.291894       76.777868            4.353857           0.316566   \n",
       "2             7.299573       76.599787            4.351565           0.316505   \n",
       "3             7.293299       76.758397            4.353607           0.316566   \n",
       "4             7.291316       76.607342            4.351662           0.316566   \n",
       "..                 ...             ...                 ...                ...   \n",
       "537           7.299104      136.439170            4.923181           0.316566   \n",
       "538           7.299540      136.345406            4.922499           0.316566   \n",
       "539           7.301684      136.539286            4.923910           0.316566   \n",
       "540           7.301448      136.494026            4.923580           0.316566   \n",
       "541           7.301957      136.517484            4.923751           0.316566   \n",
       "\n",
       "                                        Model Window Size  \n",
       "0    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "1    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "2    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "3    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "4    Finetuned_128CL_True_RoPE_tested_on_64CL        None  \n",
       "..                                        ...         ...  \n",
       "537  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "538  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "539  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "540  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "541  Finetuned_256CL_True_RoPE_tested_on_64CL        None  \n",
       "\n",
       "[542 rows x 12 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "20144b85-f5b9-4013-b952-3a9e5c674aad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MAE', 'RMSE', 'SMAPE', 'r2', 'MASE', 'Training Time',\n",
       "       'Training_time_log', 'Inference Time', 'Inference_time_log',\n",
       "       'Model memory (MB)', 'Model', 'Window Size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7df3b7c8-d740-412e-a02d-8140494cbcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MAE</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RMSE</th>\n",
       "      <th colspan=\"2\" halign=\"left\">SMAPE</th>\n",
       "      <th colspan=\"2\" halign=\"left\">r2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">MASE</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Training Time</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Inference Time</th>\n",
       "      <th>Model memory (MB)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finetuned_128CL_False_RoPE_tested_on_128CL</td>\n",
       "      <td>3.668</td>\n",
       "      <td>0.189</td>\n",
       "      <td>7.229</td>\n",
       "      <td>0.402</td>\n",
       "      <td>22.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.054</td>\n",
       "      <td>0.108</td>\n",
       "      <td>1464.059</td>\n",
       "      <td>1479.320</td>\n",
       "      <td>76.326</td>\n",
       "      <td>76.960</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finetuned_128CL_False_RoPE_tested_on_256CL</td>\n",
       "      <td>3.529</td>\n",
       "      <td>0.131</td>\n",
       "      <td>7.427</td>\n",
       "      <td>0.277</td>\n",
       "      <td>21.791</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.039</td>\n",
       "      <td>1462.988</td>\n",
       "      <td>1479.112</td>\n",
       "      <td>75.386</td>\n",
       "      <td>75.939</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finetuned_128CL_False_RoPE_tested_on_32CL</td>\n",
       "      <td>5.439</td>\n",
       "      <td>0.670</td>\n",
       "      <td>11.500</td>\n",
       "      <td>0.708</td>\n",
       "      <td>25.210</td>\n",
       "      <td>1.234</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.691</td>\n",
       "      <td>0.172</td>\n",
       "      <td>1469.785</td>\n",
       "      <td>1481.935</td>\n",
       "      <td>76.693</td>\n",
       "      <td>77.446</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finetuned_128CL_False_RoPE_tested_on_64CL</td>\n",
       "      <td>4.836</td>\n",
       "      <td>0.561</td>\n",
       "      <td>10.536</td>\n",
       "      <td>1.479</td>\n",
       "      <td>23.567</td>\n",
       "      <td>1.029</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.755</td>\n",
       "      <td>0.308</td>\n",
       "      <td>1466.388</td>\n",
       "      <td>1482.254</td>\n",
       "      <td>76.551</td>\n",
       "      <td>77.096</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_128CL</td>\n",
       "      <td>3.608</td>\n",
       "      <td>0.216</td>\n",
       "      <td>7.102</td>\n",
       "      <td>0.473</td>\n",
       "      <td>22.370</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.068</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1466.695</td>\n",
       "      <td>1593.816</td>\n",
       "      <td>76.048</td>\n",
       "      <td>81.640</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_256CL</td>\n",
       "      <td>3.601</td>\n",
       "      <td>0.148</td>\n",
       "      <td>7.523</td>\n",
       "      <td>0.351</td>\n",
       "      <td>22.403</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.056</td>\n",
       "      <td>1461.805</td>\n",
       "      <td>1632.746</td>\n",
       "      <td>75.467</td>\n",
       "      <td>77.780</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_32CL</td>\n",
       "      <td>5.600</td>\n",
       "      <td>0.805</td>\n",
       "      <td>11.877</td>\n",
       "      <td>1.061</td>\n",
       "      <td>24.876</td>\n",
       "      <td>1.489</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.909</td>\n",
       "      <td>0.310</td>\n",
       "      <td>1463.755</td>\n",
       "      <td>1537.669</td>\n",
       "      <td>76.670</td>\n",
       "      <td>78.779</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finetuned_128CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>4.898</td>\n",
       "      <td>0.585</td>\n",
       "      <td>9.126</td>\n",
       "      <td>1.426</td>\n",
       "      <td>24.304</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.352</td>\n",
       "      <td>1466.501</td>\n",
       "      <td>1539.456</td>\n",
       "      <td>76.495</td>\n",
       "      <td>78.764</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finetuned_256CL_False_RoPE_tested_on_128CL</td>\n",
       "      <td>3.764</td>\n",
       "      <td>0.152</td>\n",
       "      <td>7.756</td>\n",
       "      <td>0.541</td>\n",
       "      <td>22.754</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.057</td>\n",
       "      <td>1479.262</td>\n",
       "      <td>1540.526</td>\n",
       "      <td>135.756</td>\n",
       "      <td>136.279</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Finetuned_256CL_False_RoPE_tested_on_256CL</td>\n",
       "      <td>3.784</td>\n",
       "      <td>0.278</td>\n",
       "      <td>7.882</td>\n",
       "      <td>0.371</td>\n",
       "      <td>22.681</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.071</td>\n",
       "      <td>1478.983</td>\n",
       "      <td>1643.409</td>\n",
       "      <td>134.342</td>\n",
       "      <td>134.759</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Finetuned_256CL_False_RoPE_tested_on_32CL</td>\n",
       "      <td>7.183</td>\n",
       "      <td>0.882</td>\n",
       "      <td>13.224</td>\n",
       "      <td>1.286</td>\n",
       "      <td>28.479</td>\n",
       "      <td>1.709</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.271</td>\n",
       "      <td>1476.155</td>\n",
       "      <td>1548.370</td>\n",
       "      <td>136.749</td>\n",
       "      <td>137.148</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Finetuned_256CL_False_RoPE_tested_on_64CL</td>\n",
       "      <td>5.467</td>\n",
       "      <td>0.592</td>\n",
       "      <td>11.450</td>\n",
       "      <td>1.309</td>\n",
       "      <td>24.907</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.974</td>\n",
       "      <td>0.284</td>\n",
       "      <td>1477.296</td>\n",
       "      <td>1538.031</td>\n",
       "      <td>136.395</td>\n",
       "      <td>136.866</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_128CL</td>\n",
       "      <td>3.777</td>\n",
       "      <td>0.182</td>\n",
       "      <td>7.567</td>\n",
       "      <td>0.389</td>\n",
       "      <td>22.713</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.072</td>\n",
       "      <td>1474.526</td>\n",
       "      <td>1487.614</td>\n",
       "      <td>135.574</td>\n",
       "      <td>136.049</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_256CL</td>\n",
       "      <td>3.668</td>\n",
       "      <td>0.157</td>\n",
       "      <td>7.584</td>\n",
       "      <td>0.312</td>\n",
       "      <td>22.723</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1477.663</td>\n",
       "      <td>1491.846</td>\n",
       "      <td>134.182</td>\n",
       "      <td>134.564</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_32CL</td>\n",
       "      <td>5.204</td>\n",
       "      <td>0.564</td>\n",
       "      <td>11.938</td>\n",
       "      <td>1.391</td>\n",
       "      <td>24.619</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.042</td>\n",
       "      <td>1.892</td>\n",
       "      <td>0.301</td>\n",
       "      <td>1469.856</td>\n",
       "      <td>1487.298</td>\n",
       "      <td>136.606</td>\n",
       "      <td>137.019</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finetuned_256CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>4.385</td>\n",
       "      <td>0.501</td>\n",
       "      <td>9.079</td>\n",
       "      <td>1.406</td>\n",
       "      <td>23.687</td>\n",
       "      <td>1.029</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1.327</td>\n",
       "      <td>0.198</td>\n",
       "      <td>1475.882</td>\n",
       "      <td>1489.342</td>\n",
       "      <td>136.320</td>\n",
       "      <td>136.724</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Finetuned_32CL_False_RoPE_tested_on_128CL</td>\n",
       "      <td>4.018</td>\n",
       "      <td>0.216</td>\n",
       "      <td>7.138</td>\n",
       "      <td>0.285</td>\n",
       "      <td>23.173</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1553.583</td>\n",
       "      <td>2207.728</td>\n",
       "      <td>61.991</td>\n",
       "      <td>90.365</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Finetuned_32CL_False_RoPE_tested_on_256CL</td>\n",
       "      <td>3.530</td>\n",
       "      <td>0.167</td>\n",
       "      <td>7.073</td>\n",
       "      <td>0.225</td>\n",
       "      <td>22.483</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1556.432</td>\n",
       "      <td>2206.553</td>\n",
       "      <td>61.771</td>\n",
       "      <td>89.694</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Finetuned_32CL_False_RoPE_tested_on_32CL</td>\n",
       "      <td>5.166</td>\n",
       "      <td>0.553</td>\n",
       "      <td>10.522</td>\n",
       "      <td>0.415</td>\n",
       "      <td>24.334</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.014</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.213</td>\n",
       "      <td>1625.734</td>\n",
       "      <td>2218.965</td>\n",
       "      <td>62.428</td>\n",
       "      <td>89.761</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Finetuned_32CL_False_RoPE_tested_on_64CL</td>\n",
       "      <td>4.951</td>\n",
       "      <td>0.648</td>\n",
       "      <td>8.289</td>\n",
       "      <td>0.846</td>\n",
       "      <td>24.490</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.604</td>\n",
       "      <td>0.377</td>\n",
       "      <td>1556.777</td>\n",
       "      <td>2172.717</td>\n",
       "      <td>62.569</td>\n",
       "      <td>91.553</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Finetuned_32CL_True_RoPE_tested_on_128CL</td>\n",
       "      <td>4.161</td>\n",
       "      <td>0.329</td>\n",
       "      <td>7.277</td>\n",
       "      <td>0.429</td>\n",
       "      <td>23.508</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1985.430</td>\n",
       "      <td>2210.862</td>\n",
       "      <td>75.671</td>\n",
       "      <td>90.680</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Finetuned_32CL_True_RoPE_tested_on_256CL</td>\n",
       "      <td>3.539</td>\n",
       "      <td>0.132</td>\n",
       "      <td>7.039</td>\n",
       "      <td>0.320</td>\n",
       "      <td>22.526</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1987.645</td>\n",
       "      <td>2216.237</td>\n",
       "      <td>76.510</td>\n",
       "      <td>90.902</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Finetuned_32CL_True_RoPE_tested_on_32CL</td>\n",
       "      <td>5.386</td>\n",
       "      <td>0.626</td>\n",
       "      <td>10.697</td>\n",
       "      <td>0.558</td>\n",
       "      <td>24.538</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.018</td>\n",
       "      <td>0.301</td>\n",
       "      <td>1984.451</td>\n",
       "      <td>2211.965</td>\n",
       "      <td>77.741</td>\n",
       "      <td>91.565</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Finetuned_32CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>4.879</td>\n",
       "      <td>0.613</td>\n",
       "      <td>8.359</td>\n",
       "      <td>0.730</td>\n",
       "      <td>24.335</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.019</td>\n",
       "      <td>1.479</td>\n",
       "      <td>0.277</td>\n",
       "      <td>1984.946</td>\n",
       "      <td>2203.112</td>\n",
       "      <td>77.283</td>\n",
       "      <td>88.962</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Finetuned_64CL_True_RoPE_tested_on_128CL</td>\n",
       "      <td>4.027</td>\n",
       "      <td>0.450</td>\n",
       "      <td>7.276</td>\n",
       "      <td>0.433</td>\n",
       "      <td>23.286</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.135</td>\n",
       "      <td>1554.159</td>\n",
       "      <td>2338.083</td>\n",
       "      <td>62.225</td>\n",
       "      <td>100.665</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Finetuned_64CL_True_RoPE_tested_on_256CL</td>\n",
       "      <td>3.663</td>\n",
       "      <td>0.218</td>\n",
       "      <td>7.274</td>\n",
       "      <td>0.308</td>\n",
       "      <td>22.651</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.070</td>\n",
       "      <td>1551.272</td>\n",
       "      <td>2289.897</td>\n",
       "      <td>61.787</td>\n",
       "      <td>103.829</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Finetuned_64CL_True_RoPE_tested_on_32CL</td>\n",
       "      <td>5.210</td>\n",
       "      <td>0.436</td>\n",
       "      <td>10.603</td>\n",
       "      <td>0.600</td>\n",
       "      <td>24.502</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.952</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1546.909</td>\n",
       "      <td>2507.486</td>\n",
       "      <td>62.594</td>\n",
       "      <td>264.374</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Finetuned_64CL_True_RoPE_tested_on_64CL</td>\n",
       "      <td>4.598</td>\n",
       "      <td>0.459</td>\n",
       "      <td>8.012</td>\n",
       "      <td>0.651</td>\n",
       "      <td>24.129</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.563</td>\n",
       "      <td>0.341</td>\n",
       "      <td>1551.951</td>\n",
       "      <td>2566.094</td>\n",
       "      <td>62.513</td>\n",
       "      <td>104.783</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model    MAE           RMSE         \\\n",
       "                                                 mean    std    mean    std   \n",
       "0   Finetuned_128CL_False_RoPE_tested_on_128CL  3.668  0.189   7.229  0.402   \n",
       "1   Finetuned_128CL_False_RoPE_tested_on_256CL  3.529  0.131   7.427  0.277   \n",
       "2    Finetuned_128CL_False_RoPE_tested_on_32CL  5.439  0.670  11.500  0.708   \n",
       "3    Finetuned_128CL_False_RoPE_tested_on_64CL  4.836  0.561  10.536  1.479   \n",
       "4    Finetuned_128CL_True_RoPE_tested_on_128CL  3.608  0.216   7.102  0.473   \n",
       "5    Finetuned_128CL_True_RoPE_tested_on_256CL  3.601  0.148   7.523  0.351   \n",
       "6     Finetuned_128CL_True_RoPE_tested_on_32CL  5.600  0.805  11.877  1.061   \n",
       "7     Finetuned_128CL_True_RoPE_tested_on_64CL  4.898  0.585   9.126  1.426   \n",
       "8   Finetuned_256CL_False_RoPE_tested_on_128CL  3.764  0.152   7.756  0.541   \n",
       "9   Finetuned_256CL_False_RoPE_tested_on_256CL  3.784  0.278   7.882  0.371   \n",
       "10   Finetuned_256CL_False_RoPE_tested_on_32CL  7.183  0.882  13.224  1.286   \n",
       "11   Finetuned_256CL_False_RoPE_tested_on_64CL  5.467  0.592  11.450  1.309   \n",
       "12   Finetuned_256CL_True_RoPE_tested_on_128CL  3.777  0.182   7.567  0.389   \n",
       "13   Finetuned_256CL_True_RoPE_tested_on_256CL  3.668  0.157   7.584  0.312   \n",
       "14    Finetuned_256CL_True_RoPE_tested_on_32CL  5.204  0.564  11.938  1.391   \n",
       "15    Finetuned_256CL_True_RoPE_tested_on_64CL  4.385  0.501   9.079  1.406   \n",
       "16   Finetuned_32CL_False_RoPE_tested_on_128CL  4.018  0.216   7.138  0.285   \n",
       "17   Finetuned_32CL_False_RoPE_tested_on_256CL  3.530  0.167   7.073  0.225   \n",
       "18    Finetuned_32CL_False_RoPE_tested_on_32CL  5.166  0.553  10.522  0.415   \n",
       "19    Finetuned_32CL_False_RoPE_tested_on_64CL  4.951  0.648   8.289  0.846   \n",
       "20    Finetuned_32CL_True_RoPE_tested_on_128CL  4.161  0.329   7.277  0.429   \n",
       "21    Finetuned_32CL_True_RoPE_tested_on_256CL  3.539  0.132   7.039  0.320   \n",
       "22     Finetuned_32CL_True_RoPE_tested_on_32CL  5.386  0.626  10.697  0.558   \n",
       "23     Finetuned_32CL_True_RoPE_tested_on_64CL  4.879  0.613   8.359  0.730   \n",
       "24    Finetuned_64CL_True_RoPE_tested_on_128CL  4.027  0.450   7.276  0.433   \n",
       "25    Finetuned_64CL_True_RoPE_tested_on_256CL  3.663  0.218   7.274  0.308   \n",
       "26     Finetuned_64CL_True_RoPE_tested_on_32CL  5.210  0.436  10.603  0.600   \n",
       "27     Finetuned_64CL_True_RoPE_tested_on_64CL  4.598  0.459   8.012  0.651   \n",
       "\n",
       "     SMAPE            r2          MASE        Training Time            \\\n",
       "      mean    std   mean    std   mean    std           min       max   \n",
       "0   22.710  0.555  0.938  0.007  1.054  0.108      1464.059  1479.320   \n",
       "1   21.791  0.431  0.937  0.005  0.873  0.039      1462.988  1479.112   \n",
       "2   25.210  1.234  0.826  0.022  1.691  0.172      1469.785  1481.935   \n",
       "3   23.567  1.029  0.861  0.037  1.755  0.308      1466.388  1482.254   \n",
       "4   22.370  0.675  0.940  0.008  1.068  0.059      1466.695  1593.816   \n",
       "5   22.403  0.599  0.935  0.006  0.955  0.056      1461.805  1632.746   \n",
       "6   24.876  1.489  0.821  0.034  1.909  0.310      1463.755  1537.669   \n",
       "7   24.304  0.889  0.891  0.036  1.587  0.352      1466.501  1539.456   \n",
       "8   22.754  0.626  0.929  0.009  0.988  0.057      1479.262  1540.526   \n",
       "9   22.681  0.570  0.928  0.007  0.929  0.071      1478.983  1643.409   \n",
       "10  28.479  1.709  0.759  0.048  1.940  0.271      1476.155  1548.370   \n",
       "11  24.907  0.997  0.833  0.036  1.974  0.284      1477.296  1538.031   \n",
       "12  22.713  0.537  0.933  0.007  1.046  0.072      1474.526  1487.614   \n",
       "13  22.723  0.527  0.933  0.005  0.954  0.025      1477.663  1491.846   \n",
       "14  24.619  1.047  0.826  0.042  1.892  0.301      1469.856  1487.298   \n",
       "15  23.687  1.029  0.897  0.032  1.327  0.198      1475.882  1489.342   \n",
       "16  23.173  0.536  0.939  0.006  1.084  0.097      1553.583  2207.728   \n",
       "17  22.483  0.302  0.942  0.004  0.881  0.051      1556.432  2206.553   \n",
       "18  24.334  0.931  0.850  0.014  2.100  0.213      1625.734  2218.965   \n",
       "19  24.490  1.081  0.907  0.022  1.604  0.377      1556.777  2172.717   \n",
       "20  23.508  0.532  0.936  0.008  1.096  0.100      1985.430  2210.862   \n",
       "21  22.526  0.389  0.943  0.004  0.886  0.038      1987.645  2216.237   \n",
       "22  24.538  0.876  0.842  0.021  2.018  0.301      1984.451  2211.965   \n",
       "23  24.335  0.929  0.908  0.019  1.479  0.277      1984.946  2203.112   \n",
       "24  23.286  0.568  0.936  0.009  1.081  0.135      1554.159  2338.083   \n",
       "25  22.651  0.581  0.939  0.005  0.945  0.070      1551.272  2289.897   \n",
       "26  24.502  0.758  0.849  0.017  1.952  0.300      1546.909  2507.486   \n",
       "27  24.129  0.698  0.916  0.016  1.563  0.341      1551.951  2566.094   \n",
       "\n",
       "   Inference Time          Model memory (MB)  \n",
       "              min      max              mean  \n",
       "0          76.326   76.960             0.316  \n",
       "1          75.386   75.939             0.316  \n",
       "2          76.693   77.446             0.316  \n",
       "3          76.551   77.096             0.316  \n",
       "4          76.048   81.640             0.317  \n",
       "5          75.467   77.780             0.317  \n",
       "6          76.670   78.779             0.317  \n",
       "7          76.495   78.764             0.317  \n",
       "8         135.756  136.279             0.316  \n",
       "9         134.342  134.759             0.316  \n",
       "10        136.749  137.148             0.316  \n",
       "11        136.395  136.866             0.316  \n",
       "12        135.574  136.049             0.317  \n",
       "13        134.182  134.564             0.317  \n",
       "14        136.606  137.019             0.317  \n",
       "15        136.320  136.724             0.317  \n",
       "16         61.991   90.365             0.316  \n",
       "17         61.771   89.694             0.316  \n",
       "18         62.428   89.761             0.316  \n",
       "19         62.569   91.553             0.316  \n",
       "20         75.671   90.680             0.317  \n",
       "21         76.510   90.902             0.317  \n",
       "22         77.741   91.565             0.317  \n",
       "23         77.283   88.962             0.317  \n",
       "24         62.225  100.665             0.317  \n",
       "25         61.787  103.829             0.317  \n",
       "26         62.594  264.374             0.317  \n",
       "27         62.513  104.783             0.317  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if exp_name != \"exp3\":\n",
    "\n",
    "    all_models_df = all_models_df.dropna()\n",
    "\n",
    "    summary = all_models_df.groupby(['Model', 'Window Size']).agg({\n",
    "        'MAE': ['mean', 'std'],\n",
    "        'RMSE': ['mean', 'std'],\n",
    "        'SMAPE': ['mean', 'std'],\n",
    "        'r2': ['mean', 'std'],\n",
    "        'MASE': ['mean', 'std'],\n",
    "        'Training Time': ['mean'],\n",
    "        'Inference Time': ['mean'],\n",
    "        'Model memory (MB)': ['mean'],\n",
    "    }).reset_index()\n",
    "\n",
    "    summary = summary.round(3)\n",
    "\n",
    "    summary.columns = ['_'.join(col).strip() if col[1] else col[0] for col in summary.columns.values]\n",
    "\n",
    "\n",
    "    summary = summary.rename(columns={\n",
    "        'Model_': 'Model',\n",
    "        'Window Size_': 'Window Size',\n",
    "        'MAE_mean': 'MAE_mean',\n",
    "        'MAE_std': 'MAE_std',\n",
    "        'RMSE_mean': 'RMSE_mean',\n",
    "        'RMSE_std': 'RMSE_std',\n",
    "        'SMAPE_mean': 'SMAPE_mean',\n",
    "        'SMAPE_std': 'SMAPE_std',\n",
    "        'r2_mean': 'r2_mean',\n",
    "        'r2_std': 'r2_std',\n",
    "        'MASE_mean': 'MASE_mean',\n",
    "        'MASE_std': 'MASE_std',\n",
    "        'Training Time_mean': 'Training Time',\n",
    "        'Inference Time_mean': 'Inference Time',\n",
    "        'Model memory (MB)_mean': 'Model memory (MB)'\n",
    "    })\n",
    "\n",
    "    summary.to_csv(f\"../../outputs/{exp_name}_model_metrics_avg.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    \n",
    "    summary = all_models_df.groupby(['Model']).agg({\n",
    "        'MAE': ['mean', 'std'],\n",
    "        'RMSE': ['mean', 'std'],\n",
    "        'SMAPE': ['mean', 'std'],\n",
    "        'r2': ['mean', 'std'],\n",
    "        'MASE': ['mean', 'std'],\n",
    "        'Training Time': ['min', 'max'],#['mean'],\n",
    "        'Inference Time': ['min', 'max'],#['mean'],\n",
    "        'Model memory (MB)': ['mean'],\n",
    "    }).reset_index()\n",
    "\n",
    "    summary = summary.round(3)\n",
    "    summary.to_csv(f\"../../outputs/{exp_name}_model_metrics_avg.csv\", index=False)\n",
    "\n",
    "summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cd7ebed8-98b5-41dc-96e4-5aa9a4e3de05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time  min    1570.388857\n",
      "               max    1850.087250\n",
      "dtype: float64\n",
      "Inference Time  min     89.308679\n",
      "                max    107.533750\n",
      "dtype: float64\n",
      "Model memory (MB)  mean    0.316571\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(summary[[\"Training Time\"]].mean())\n",
    "print(summary[[\"Inference Time\"]].mean())\n",
    "print(summary[[\"Model memory (MB)\"]].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARF_paper",
   "language": "python",
   "name": "arf_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
